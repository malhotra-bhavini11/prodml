# -*- coding: utf-8 -*-
"""Titanic_Survival_ipynb (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_OMsyIPahA762fY4deS2byWqx482Z0ao
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

train_df_raw = pd.read_csv('/content/train.csv')

test_df_raw = pd.read_csv('/content/test.csv')

train_df_raw.head()

test_df_raw.head()



# Convert 'Pclass' to object type
train_df_raw['Pclass'] = train_df_raw['Pclass'].astype(str)
test_df_raw['Pclass'] = test_df_raw['Pclass'].astype(str)

train_df_raw.info()

test_df_raw.info()

df = train_df_raw.drop('Ticket', axis=1)
test_df = test_df_raw.drop('Ticket', axis=1)

combine = [df, test_df]

df.head(20)

df.describe()

df.info()

df_num = df[['Age', 'SibSp', 'Parch', 'Fare']]
df_cat = df[['Survived', 'Pclass', 'Sex', 'Cabin', 'Embarked']]

# Set up the seaborn style
sns.set(style="whitegrid")

# Create a subplot grid
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))

# Flatten the 2D array of subplots
axes = axes.flatten()

# Use a for loop to create distribution plots for each column
for i, column in enumerate(df_num.columns):
    sns.histplot(df_num[column], kde=True, bins=20, color='skyblue', ax=axes[i])
    axes[i].set_title(f'Distribution of {column}')
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Count')

# Adjust layout
plt.tight_layout()
plt.show()

# Calculate the correlation matrix
correlation_matrix = df_num.corr()

# Set up the seaborn style
sns.set(style="whitegrid")

# Create a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix Heatmap')
plt.show()

"""###Percentage Survived Breakdown (By Sex)"""

# Use groupby and value_counts to calculate absolute counts
result = df.groupby('Sex')['Survived'].value_counts().unstack()

# Display the result
print(result)

# Use groupby and value_counts to calculate percentages
result = df.groupby('Sex')['Survived'].value_counts(normalize=True).unstack() * 100

# Display the result
print(result)

"""###Percentage Survived Breakdown (By Class)



"""

# Use groupby and value_counts to calculate absolute counts
result = df.groupby('Pclass')['Survived'].value_counts().unstack()

# Display the result
print(result)

# Use groupby and value_counts to calculate percentages
result = df.groupby('Pclass')['Survived'].value_counts(normalize=True).unstack() * 100

# Display the result
print(result)

"""###Percentage Survived Breakdown (By Siblings & Spouses)

"""

# Use groupby and value_counts to calculate absolute counts
result = df.groupby('SibSp')['Survived'].value_counts().unstack()

# Display the result
print(result)

# Use groupby and value_counts to calculate percentages
result = df.groupby('SibSp')['Survived'].value_counts(normalize=True).unstack() * 100

# Display the result
print(result)

"""###Percentage Survived Breakdown (By Parents & Children)"""

# Use groupby and value_counts to calculate absolute counts
result = df.groupby('Parch')['Survived'].value_counts().unstack()

# Display the result
print(result)

# Use groupby and value_counts to calculate percentages
result = df.groupby('Parch')['Survived'].value_counts(normalize=True).unstack() * 100

# Display the result
print(result)

"""###Percentage Survived Breakdown(By Cabin)"""

# Extract the letter from the 'Cabin' column and impute null values with 'n'
df['CabinLetter'] = df['Cabin'].str.extract('([A-Za-z])').fillna('n')

# Display the count of each letter
letter_counts = df['CabinLetter'].value_counts()

# Display the result
print(letter_counts)

# Iterate through the combined dataframes
for dataset in combine:
    # Extract the letter from the 'Cabin' column and impute null values with 'n'
    dataset['CabinLetter'] = dataset['Cabin'].str.extract('([A-Za-z])').fillna('n')

# Extract the letter from the 'Cabin' column and impute null values with 'n'
df['CabinLetter'] = df['Cabin'].str.extract('([A-Za-z])').fillna('n')

# Group by 'CabinLetter' and count the occurrences of 1s and 0s in 'Survived'
survival_breakdown = df.groupby(['CabinLetter', 'Survived']).size().unstack()

# Display the result
print(survival_breakdown)

"""###Title & Survival Chance

"""

# Extract titles using regular expressions
df['Title'] = df['Name'].str.extract(r',\s*([^\.]+)\.')

# Display the count of each title
title_counts = df['Title'].value_counts()

# Display the result
print(title_counts)

# Iterate through the combined dataframes
for dataset in combine:
    # Extract titles using regular expressions
    dataset['Title'] = dataset['Name'].str.extract(r',\s*([^\.]+)\.')

# Extract titles using regular expressions
df['Title'] = df['Name'].str.extract(r',\s*([^\.]+)\.')

# Extract cabin letters and impute null values with 'n'
df['CabinLetter'] = df['Cabin'].str.extract('([A-Za-z])').fillna('n')

df = df.drop(['Name', 'Cabin'], axis=1)

for dataset in combine:
    # Extract titles using regular expressions
    dataset['Title'] = dataset['Name'].str.extract(r',\s*([^\.]+)\.')

    # Extract cabin letters and impute null values with 'n'
    dataset['CabinLetter'] = dataset['Cabin'].str.extract('([A-Za-z])').fillna('n')

    # Drop unnecessary columns
    dataset.drop(['Name', 'Cabin'], axis=1, inplace=True)

df.head(10)

# Calculate survival percentage for each title
survival_percentage_by_title = df.groupby('Title')['Survived'].mean() * 100

# Display the result
print(survival_percentage_by_title)

# Display 0 and 1 counts from the 'Survived' column grouped by title
survival_counts_by_title = df.groupby(['Title', 'Survived']).size().unstack(fill_value=0)

# Display the result
print(survival_counts_by_title)

"""###Survival rate correlated with each categorical variable

"""

# Select categorical columns
df_cat = df[['Survived', 'Pclass', 'Sex', 'CabinLetter', 'Embarked']]

# Set up the seaborn style
sns.set(style="whitegrid")

# Create plots for each categorical variable grouped by 'Survived'
for column in df_cat.columns[1:]:
    plt.figure(figsize=(8, 5))
    sns.countplot(x=column, hue='Survived', data=df_cat, palette='pastel', edgecolor=".6")
    plt.title(f'Survival Count by {column}')
    plt.xlabel(column)
    plt.ylabel('Count')
    plt.legend(title='Survived', loc='upper right', labels=['No', 'Yes'])
    plt.show()

"""###Imputing Missing Values"""

#Using median for each title

# Calculate median age for each title
median_age_by_title = df.groupby('Title')['Age'].median()
print (median_age_by_title)

# Impute missing values in 'Age' column with the median corresponding to each title
df['Age'] = df.apply(lambda row: median_age_by_title[row['Title']] if pd.isnull(row['Age']) else row['Age'], axis=1)

# Display the DataFrame with imputed values
print(df)

# Iterate through the combined dataframes
for dataset in combine:
    # Calculate median age for each title
    median_age_by_title = dataset.groupby('Title')['Age'].median()

    # Impute missing values in 'Age' column with the median corresponding to each title
    dataset['Age'] = dataset.apply(lambda row: median_age_by_title[row['Title']] if pd.isnull(row['Age']) else row['Age'], axis=1)

#Using mean for each title

# Calculate mean age for each title
mean_age_by_title = df.groupby('Title')['Age'].mean()
print (mean_age_by_title)

# Impute missing values in 'Age' column with the mean corresponding to each title
df['Age'] = df.apply(lambda row: mean_age_by_title[row['Title']] if pd.isnull(row['Age']) else row['Age'], axis=1)

# Display the DataFrame with imputed values
print(df)

# Iterate through the combined dataframes
for dataset in combine:
    # Calculate mean age for each title
    mean_age_by_title = dataset.groupby('Title')['Age'].mean()

    # Impute missing values in 'Age' column with the mean corresponding to each title
    dataset['Age'] = dataset.apply(lambda row: mean_age_by_title[row['Title']] if pd.isnull(row['Age']) else row['Age'], axis=1)

df.info()

# Drop rows with null values in the 'Embarked' column
df = df.dropna(subset=['Embarked'])

df.info()

test_df.info()

shallow_copy_df = test_df.copy()

# Assuming 'test_df' is already defined
rows_with_null_values = shallow_copy_df[shallow_copy_df.isnull().any(axis=1)]

# Display the rows with null values
print(rows_with_null_values)

# Assuming 'test_df' is already defined
# Calculate median age for each combination of 'Sex' and 'Pclass'
median_age_by_sex_pclass = test_df.groupby(['Sex', 'Pclass'])['Age'].median()

# Calculate median fare for each combination of 'Pclass' and 'Embarked'
median_fare_by_pclass_embarked = test_df.groupby(['Pclass', 'Embarked'])['Fare'].median()

# Create a boolean mask for rows with missing values in 'Age'
missing_age_mask = test_df['Age'].isnull()

# Fill missing values in 'Age' with the median corresponding to 'Sex' and 'Pclass'
test_df.loc[missing_age_mask, 'Age'] = test_df.loc[missing_age_mask].apply(
    lambda row: median_age_by_sex_pclass.loc[row['Sex'], row['Pclass']],
    axis=1
)

# Create a boolean mask for rows with missing values in 'Fare'
missing_fare_mask = test_df['Fare'].isnull()

# Fill missing values in 'Fare' with the median corresponding to 'Pclass' and 'Embarked'
test_df.loc[missing_fare_mask, 'Fare'] = test_df.loc[missing_fare_mask].apply(
    lambda row: median_fare_by_pclass_embarked.loc[row['Pclass'], row['Embarked']],
    axis=1
)

# Display the rows that have been filled
filled_rows = test_df.loc[missing_age_mask | missing_fare_mask]
print(filled_rows)

test_df.info()

"""###Model creation

###Logistic Regression
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Separate features and target variable
X = df[['Age', 'Embarked', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'CabinLetter']]
y = df['Survived']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Separate features from test_df
X_test_df = test_df[['Age', 'Embarked', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'CabinLetter']]

#Without using Cabin and Fare
# Create a preprocessing pipeline for numeric and categorical features
numeric_features = ['Age', 'SibSp', 'Parch']
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_features = ['Embarked', 'Sex']
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create a logistic regression model
log_model1 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', LogisticRegression(random_state=42))])

# Train the model
log_model1.fit(X_train, y_train)

# Predict on the test set
y_pred = log_model1.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")



#Using Cabin and Fare
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a preprocessing pipeline for numeric and categorical features
numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_features = ['Embarked', 'Sex', 'CabinLetter']
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create a logistic regression model
log_model2 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', LogisticRegression(random_state=42))])

# Train the model
log_model2.fit(X_train, y_train)

# Predict on the test set
y_pred = log_model2.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")



"""###Decision Tree"""

#Without Using Cabin and Fare

from sklearn.tree import DecisionTreeClassifier
# Separate features and target variable
X = df[['Age', 'Embarked', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'CabinLetter']]
y = df['Survived']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a preprocessing pipeline for numeric and categorical features
numeric_features = ['Age', 'SibSp', 'Parch']
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_features = ['Embarked', 'Pclass', 'Sex']
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create a decision tree model
DT_model1 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', DecisionTreeClassifier(random_state=42))])

# Train the model
DT_model1.fit(X_train, y_train)

# Predict on the test set
y_pred = DT_model1.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

#Using Cabin and Fare

from sklearn.tree import DecisionTreeClassifier
# Separate features and target variable
X = df[['Age', 'Embarked', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'CabinLetter']]
y = df['Survived']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a preprocessing pipeline for numeric and categorical features
numeric_features = ['Age', 'SibSp', 'Parch', 'Fare']
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_features = ['Embarked', 'Pclass', 'Sex', 'CabinLetter']
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create a decision tree model
DT_model2 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', DecisionTreeClassifier(random_state=42))])

# Train the model
DT_model2.fit(X_train, y_train)

# Predict on the test set
y_pred = DT_model2.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

"""###Support Vector Classifier"""

#Without Using Cabin and Fare
from sklearn.svm import SVC

# Create a preprocessing pipeline for numeric and categorical features
numeric_features = ['Age', 'SibSp', 'Parch']
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_features = ['Embarked', 'Pclass', 'Sex']
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create an SVC model
SVC_model1 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', SVC(random_state=42))])

# Train the model
SVC_model1.fit(X_train, y_train)

# Predict on the test set
y_pred = SVC_model1.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

#Using Cabin and Fare
from sklearn.svm import SVC

# Create a preprocessing pipeline for numeric and categorical features
numeric_features = ['Age', 'SibSp', 'Parch','Fare']
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_features = ['Embarked', 'Pclass', 'Sex','CabinLetter']
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create an SVC model
SVC_model2 = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', SVC(random_state=42))])

# Train the model
SVC_model2.fit(X_train, y_train)

# Predict on the test set
y_pred = SVC_model2.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

"""#Prediction on Test Data"""

#Logistic Regression Prediction on Test Data

# Predict on the test set
y_pred_test_df_log1 = log_model1.predict(X_test_df)

# Display the predictions
print("Predictions for test_df:")
print(y_pred_test_df_log1)

y_pred_test_df_log2 = log_model2.predict(X_test_df)

# Display the predictions
print("Predictions for test_df:")
print(y_pred_test_df_log2)

#Decision Tree Prediction on Test Data

# Predict on the test set
y_pred_test_df_DT1 = DT_model1.predict(X_test_df)

# Display the predictions
print("Predictions for test_df:")
print(y_pred_test_df_DT1)

# Predict on the test set
y_pred_test_df_DT2 = DT_model2.predict(X_test_df)

# Display the predictions
print("Predictions for test_df:")
print(y_pred_test_df_DT2)

#SVC Prediction on Test Data

# Predict on the test set
y_pred_test_df_SVC1 = SVC_model1.predict(X_test_df)

# Display the predictions
print("Predictions for test_df:")
print(y_pred_test_df_SVC1)

#SVC Prediction on Test Data

# Predict on the test set
y_pred_test_df_SVC2 = SVC_model2.predict(X_test_df)

# Display the predictions
print("Predictions for test_df:")
print(y_pred_test_df_SVC2)

from sklearn.ensemble import VotingClassifier
# Set probability=True for classifiers that support it
SVC_model1.probability = True
log_model1.probability = True

# Create a hard voting classifier
voting_clf1 = VotingClassifier(
    estimators=[
        ('svc', SVC_model1),
        ('logistic', log_model1),
        ('decision_tree', DT_model1)
    ],
    voting='hard'  # Use 'hard' for hard voting
)

# Fit the hard voting classifier on the training data
voting_clf1.fit(X_train, y_train)

# Predict on the training set
y_pred_train = voting_clf1.predict(X_train)

# Evaluate the model on the training set
accuracy_train = accuracy_score(y_train, y_pred_train)
print(f"Accuracy on Training Set: {accuracy_train}")

from sklearn.ensemble import VotingClassifier
# Set probability=True for classifiers that support it
SVC_model2.probability = True
log_model2.probability = True

# Create a hard voting classifier
voting_clf2 = VotingClassifier(
    estimators=[
        ('svc', SVC_model2),
        ('logistic', log_model2),
        ('decision_tree', DT_model2)
    ],
    voting='hard'  # Use 'hard' for hard voting
)

# Fit the hard voting classifier on the training data
voting_clf2.fit(X_train, y_train)

# Predict on the training set
y_pred_train = voting_clf2.predict(X_train)

# Evaluate the model on the training set
accuracy_train = accuracy_score(y_train, y_pred_train)
print(f"Accuracy on Training Set: {accuracy_train}")

import joblib

# Assuming X_train, y_train are already defined and preprocessed
# Train your model
voting_clf2.fit(X_train, y_train)

# Save the model to a file
joblib.dump(voting_clf2, 'voting_clf_model.pkl')
print("Model saved successfully!")

#Voting classifier prediction on test data
y_pred_test_df_clf1 = voting_clf1.predict(X_test_df)

# Display the predictions
print("Predictions for test_df:")
print(y_pred_test_df_clf1)



#Voting classifier prediction on test data
y_pred_test_df_clf2 = voting_clf2.predict(X_test_df)

# Display the predictions
print("Predictions for test_df:")
print(y_pred_test_df_clf2)

# Assuming 'y_pred_test_df_clf1' and 'test_df' are already defined
from google.colab import files

# Create a DataFrame with PassengerId and the corresponding predictions
result_df_clf1 = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': y_pred_test_df_clf1})

# Save the DataFrame to a CSV file
result_df_clf1.to_csv('predictions.csv', index=False)

# Download the CSV file
files.download('predictions.csv')

# Assuming 'y_pred_test_df_clf1' and 'test_df' are already defined
from google.colab import files

# Create a DataFrame with PassengerId and the corresponding predictions
result_df_clf2 = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': y_pred_test_df_clf2})

# Save the DataFrame to a CSV file
result_df_clf2.to_csv('predictions2.csv', index=False)

# Download the CSV file
files.download('predictions2.csv')
